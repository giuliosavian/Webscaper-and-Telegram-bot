{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trial for scraping web "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "#scraping\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from threading import Timer\n",
    "import sched\n",
    "import requests\n",
    "from queue import Queue\n",
    "#data analyses\n",
    "#import tensorflow as tf\n",
    "#import cv2\n",
    "from statistics import mean\n",
    "import pandas as pd\n",
    "from skimage import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INPUT\n",
    "# @Link : vinted, subito, ...\n",
    "# @Search: what I want to search\n",
    "\n",
    "#OUTPUT\n",
    "# @Products, prices, categories, images, links\n",
    "\n",
    "global site, product \n",
    "\n",
    "global old_products_fifo\n",
    "\n",
    "def web_scraper(Link,Search):\n",
    "    \n",
    "    global products, prices, categories, images, links, product_data\n",
    "\n",
    "    products    = []    #List to store name of the product\n",
    "    prices      = []    #List to store price of the product\n",
    "    categories  = []    #List to store rating of the product\n",
    "    images     = []    #List of Immages\n",
    "    links       = []    #List of Links\n",
    "    product_data = [] #Is a list containing lists [categories,products,prices,images,links]\n",
    "\n",
    "    #select the web browser\n",
    "    driver = webdriver.Chrome(\"/usr/lib/chromium-browser/chromedriver\")\n",
    "\n",
    "    #Will navigate to a page given by the URL forget\n",
    "    if Link == 0: #Vinted\n",
    "        baselink = 'https://www.vinted.it/catalog?search_text='\n",
    "        filters = '&order=newest_first'\n",
    "        driver.get(baselink + Search + filters)\n",
    "\n",
    "    elif Link == 1: #Subito\n",
    "        baselink = 'https://www.vinted.it/catalog?search_text='\n",
    "        filters = '&order=newest_first'\n",
    "        driver.get(baselink + Search +  filters)\n",
    "\n",
    "    elif Link == 2: #Marketplace\n",
    "        baselink = 'https://www.vinted.it/catalog?search_text='\n",
    "        filters = '&order=newest_first'\n",
    "        driver.get(baselink + Search +  filters)\n",
    "    else:\n",
    "        Err_case = 1\n",
    "    \n",
    "    #get source code \n",
    "    content = driver.page_source\n",
    "    \n",
    "    soup = BeautifulSoup(content)\n",
    "    soup.prettify()\n",
    "\n",
    "    #scraping the page\n",
    "    for page in soup.findAll('div', attrs={\"class\" : \"feed-grid\"}):\n",
    "\n",
    "        #scraping the boxes\n",
    "        for box in page.findAll('div', attrs={\"class\" : \"web_ui__ItemBox__box\"}):\n",
    "\n",
    "            #scraping the name the prize and the link for on a single box\n",
    "            advertisement = []\n",
    "\n",
    "            #CATEGORY\n",
    "            category = box.find('div', attrs={'class':'web_ui__ItemBox__details'})\n",
    "            categories.append(category.text)\n",
    "            advertisement.append(category.text)\n",
    "            #print(category.text)\n",
    "\n",
    "            #PRICE\n",
    "            price = box.find('div', attrs={'web_ui__ItemBox__title-content'})\n",
    "            price_raw = price.text\n",
    "            price_clean = price_raw.replace('\\xa0', '')\n",
    "            prices.append(price_clean)\n",
    "            advertisement.append(price_clean)\n",
    "            \n",
    "            #LINK\n",
    "            tmp = box.find('div', attrs={'web_ui__ItemBox__image-container'})\n",
    "            link = tmp.find('a')\n",
    "            links.append(link.get('href'))\n",
    "            advertisement.append(link.get('href'))\n",
    "\n",
    "            #SUBCATEGORY\n",
    "\n",
    "            #IMMAGE and PRODUCT\n",
    "            tmp = box.find('div', attrs={'web_ui__Image__image web_ui__Image__cover web_ui__Image__portrait web_ui__Image__scaled web_ui__Image__ratio'})\n",
    "            image_link  = tmp.find('img')\n",
    "            products.append(image_link.get('alt'))\n",
    "            image = io.imread(image_link.get('src'))\n",
    "            images.append(image)\n",
    "            #print(image.get('alt'))\n",
    "            #print(image.get('src'))\n",
    "            advertisement.append(image_link.get('alt'))\n",
    "            #advertisement.append(image) #Wrong\n",
    "            product_data.append(advertisement) # contanins all the data grupped \n",
    "\n",
    "    #Reduce lenth\n",
    "    products = products[0:10]\n",
    "    prices = prices[0:10]\n",
    "    categories = categories[0:10]\n",
    "    links = links[0:10]\n",
    "    images = images[0:10]\n",
    "    product_data = product_data[0:10]\n",
    "    #print(product_data)\n",
    "    return products, prices, categories, links, images, product_data\n",
    "        \n",
    "def Web_init():\n",
    "    #Define site and product \n",
    "    print(\"Insert the wanted site:\")\n",
    "    site = int(input(\"[0: Vinted ; 1:Subito ; 2 Marketplace]\\n\"))\n",
    "    print(site)\n",
    "    \n",
    "    print(\"Insert the wanted product:\")\n",
    "    product = input(\"Insert the wanted product:\\n\")  \n",
    "    print(product)\n",
    "\n",
    "    return site, product\n",
    "\n",
    "#def refresh():\n",
    "#    driver.refresh()\n",
    "\n",
    "def isnew(products_data, old_products):\n",
    "    potential_products = []\n",
    "    #print(products_data)\n",
    "    #print(\"old product:\")\n",
    "    #print(old_products)\n",
    "    for list in products_data:\n",
    "        list[3]\n",
    "        if list[3] not in old_products.queue:            \n",
    "            potential_products.append(list)\n",
    "            old_products.put(list[3])\n",
    "    return potential_products\n",
    "\n",
    "def scrape_prices_ebay(product_list):\n",
    "    prices = {}\n",
    "    for product in product_list:\n",
    "        product = product[3]\n",
    "        price = []\n",
    "        #print(f\"The product under analysis is: {product}\")\n",
    "        # format the search query\n",
    "        search_query = product.replace(' ', '+')\n",
    "        # send a request to eBay's search page\n",
    "        response = requests.get(f'https://www.ebay.com/sch/i.html?_nkw={search_query}&LH_Complete=1&LH_Sold=1')\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        soup.prettify()\n",
    "        # extract the first result from the search results \n",
    "        First = True\n",
    "        #try:\n",
    "        num = 0\n",
    "        for result in soup.findAll('span', attrs={'class': 's-item__price'}):\n",
    "\n",
    "            #print(f'Scraped string,{result}')\n",
    "            if First == True:\n",
    "                #print('First discrarded')\n",
    "                First = False\n",
    "                continue\n",
    "            else:\n",
    "                if \"to\" in result.text:\n",
    "                    #print('Contains (to)')\n",
    "                    continue\n",
    "                else:\n",
    "                    str = result.text\n",
    "                    num = str.replace(\"$\", \"\")\n",
    "                    num = num.replace(\"€\", \"\")\n",
    "                    num = float(num.replace(\",\", \"\"))\n",
    "                    num_round = round(num,2)\n",
    "                    price.append(num_round)\n",
    "                    #print(f'Single price,{price}')\n",
    "\n",
    "        if len(price)>0:\n",
    "            average_price = mean(price)\n",
    "            max = average_price*10\n",
    "            min = average_price/10\n",
    "\n",
    "        for i in price:\n",
    "            if i > max or i< min:\n",
    "                price.remove(i)\n",
    "\n",
    "        if len(price)>0:\n",
    "            average_price = mean(price)\n",
    "\n",
    "\n",
    "        #print(f'Average price,{average_price}')\n",
    "        prices[product] = average_price\n",
    "        \n",
    "    print(f'Number of potential products:{len(product_list)}')\n",
    "        #except:\n",
    "        #    print(f'Error scrape_prices_ebay,{str}')\n",
    "        #    prices[product] = 'Not found'\n",
    "            \n",
    "    return prices\n",
    "\n",
    "def compare_prices(marketprices,potential_products):\n",
    "    m_prices = list(marketprices.values())\n",
    "    good_indeces = []\n",
    "    bad_indeces = []\n",
    "    count = 0\n",
    "\n",
    "    for m_price in m_prices:\n",
    "        product = potential_products[count]\n",
    "        price = product[1]\n",
    "        tmp = price.replace(\"$\", \"\")\n",
    "        tmp = tmp.replace(\"€\", \"\")\n",
    "        tmp = float(tmp.replace(\",\", \"\"))\n",
    "\n",
    "        if tmp * 0.7 < m_price:\n",
    "            good_indeces.append(count)\n",
    "        else:\n",
    "            bad_indeces.append(count)\n",
    "            \n",
    "        count +=1\n",
    "\n",
    "    #Repo\n",
    "    print(f'N. good product:{len(good_indeces)}')\n",
    "    \n",
    "    return good_indeces, bad_indeces\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    # Create a scheduler instance\n",
    "    sc = sched.scheduler()\n",
    "\n",
    "    sc.enter(10, 1, web_scraper, (site, product) )\n",
    "    # Run the scheduler\n",
    "    sc.run()\n",
    "\n",
    "    #Check new products\n",
    "    potential_products = isnew(product_data, old_products_fifo)\n",
    "    #print(f\"New products inserted:{potential_products}\")\n",
    "\n",
    "    #scrape the new products prizes on the web\n",
    "    marketprices = scrape_prices_ebay(potential_products)\n",
    "    #print(f\"Market price calculated:{marketprices}\")\n",
    "    \n",
    "    #discard the object not founded\n",
    "    #for marketprice in marketprices:\n",
    "    #    if marketprice == 'Not found':\n",
    "    #        marketprices.remove(marketprice)\n",
    "    \n",
    "    #print(f\"Price:{marketprices}\")\n",
    "    \n",
    "    #compare product price\n",
    "    good_indeces, bad_indeces = compare_prices(marketprices,potential_products)\n",
    "    \n",
    "    #generate advertisement for good indeces\n",
    "    m_prices = list(marketprices.values())\n",
    "    for i in good_indeces:\n",
    "        print(f\"Value:{potential_products[i]}\")\n",
    "        print(f\"Market price:{m_prices[i]}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call the function\n",
    "\n",
    "##WEB Init\n",
    "[site, product] = Web_init()\n",
    "web_scraper(site, product)\n",
    "\n",
    "old_products = []\n",
    "old_products_fifo = Queue(1000)\n",
    "\n",
    "while True:\n",
    "\n",
    "    try:\n",
    "        main()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'Error main: {e}')    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
